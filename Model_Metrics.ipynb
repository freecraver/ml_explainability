{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pyparsing.py:3174: FutureWarning: Possible set intersection at position 3\n",
      "  self.re = re.compile(self.reString)\n"
     ]
    }
   ],
   "source": [
    "import autosklearn.classification\n",
    "import pandas as pd\n",
    "from joblib import dump, load\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for model_path in glob.glob(\"model_3cv/model_run_*\"):\n",
    "    models.append(load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_models = []\n",
    "for model_path in glob.glob(\"model_3cv/bad_auto_model_run_*\"):\n",
    "    bad_models.append(load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_conv_cols = ['age', 'age_o', 'importance_same_race', 'importance_same_religion', 'pref_o_attractive', 'pref_o_sincere',\\\n",
    "                    'pref_o_intelligence', 'pref_o_funny', 'pref_o_ambitious', 'pref_o_shared_interests', 'attractive_o', 'sinsere_o', \\\n",
    "                    'intelligence_o', 'funny_o', 'ambitous_o', 'shared_interests_o', 'attractive_important', 'sincere_important', \\\n",
    "                    'intellicence_important', 'funny_important', 'ambtition_important', 'shared_interests_important', \\\n",
    "                    'attractive', 'sincere', 'intelligence', 'funny', 'ambition', 'attractive_partner', 'sincere_partner', \\\n",
    "                     'intelligence_partner', 'funny_partner', 'ambition_partner', 'shared_interests_partner', 'sports', \\\n",
    "                     'tvsports', 'exercise', 'dining', 'museums', 'art', 'hiking', 'gaming', 'clubbing', 'reading', 'tv', \\\n",
    "                     'theater', 'movies', 'concerts', 'music', 'shopping', 'yoga', 'interests_correlate', 'expected_happy_with_sd_people', \\\n",
    "                    'expected_num_interested_in_me', 'expected_num_matches', 'like', 'guess_prob_liked', 'met', 'gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"speeddating_reduced.csv\")\n",
    "X= df[numeric_conv_cols]\n",
    "y= df['match']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = []\n",
    "for model in models+bad_models:\n",
    "    y_preds.append(model.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6006  992]\n",
      " [ 315 1065]]\n",
      "[[6176  822]\n",
      " [ 317 1063]]\n",
      "[[5778 1220]\n",
      " [ 304 1076]]\n",
      "[[6824  174]\n",
      " [ 754  626]]\n",
      "[[6877  121]\n",
      " [ 793  587]]\n",
      "[[6903   95]\n",
      " [ 843  537]]\n"
     ]
    }
   ],
   "source": [
    "for y_pred in y_preds:\n",
    "    print(confusion_matrix(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAL_ACC: 0.8149921716763796 ACC: 0.8439961804726666 F1: 0.6197265056735525\n",
      "BAL_ACC: 0.8264138615173693 ACC: 0.864048698973502 F1: 0.6511485451761102\n",
      "BAL_ACC: 0.8026873102459916 ACC: 0.8180950107424206 F1: 0.5854189336235038\n",
      "BAL_ACC: 0.7143794707390517 ACC: 0.8892337073287181 F1: 0.5743119266055047\n",
      "BAL_ACC: 0.7040358321839366 ACC: 0.890904750537121 F1: 0.5622605363984674\n",
      "BAL_ACC: 0.6877775637759856 ACC: 0.8880401050370017 F1: 0.5337972166998011\n"
     ]
    }
   ],
   "source": [
    "for y_pred in y_preds:\n",
    "    print('BAL_ACC:', balanced_accuracy_score(y, y_pred),\n",
    "          'ACC:', accuracy_score(y, y_pred),\n",
    "          'F1:', f1_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.86      0.90      6998\n",
      "           1       0.52      0.77      0.62      1380\n",
      "\n",
      "    accuracy                           0.84      8378\n",
      "   macro avg       0.73      0.81      0.76      8378\n",
      "weighted avg       0.88      0.84      0.86      8378\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.88      0.92      6998\n",
      "           1       0.56      0.77      0.65      1380\n",
      "\n",
      "    accuracy                           0.86      8378\n",
      "   macro avg       0.76      0.83      0.78      8378\n",
      "weighted avg       0.89      0.86      0.87      8378\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.83      0.88      6998\n",
      "           1       0.47      0.78      0.59      1380\n",
      "\n",
      "    accuracy                           0.82      8378\n",
      "   macro avg       0.71      0.80      0.73      8378\n",
      "weighted avg       0.87      0.82      0.83      8378\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.98      0.94      6998\n",
      "           1       0.78      0.45      0.57      1380\n",
      "\n",
      "    accuracy                           0.89      8378\n",
      "   macro avg       0.84      0.71      0.76      8378\n",
      "weighted avg       0.88      0.89      0.88      8378\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.98      0.94      6998\n",
      "           1       0.83      0.43      0.56      1380\n",
      "\n",
      "    accuracy                           0.89      8378\n",
      "   macro avg       0.86      0.70      0.75      8378\n",
      "weighted avg       0.89      0.89      0.88      8378\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.99      0.94      6998\n",
      "           1       0.85      0.39      0.53      1380\n",
      "\n",
      "    accuracy                           0.89      8378\n",
      "   macro avg       0.87      0.69      0.74      8378\n",
      "weighted avg       0.88      0.89      0.87      8378\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for y_pred in y_preds:\n",
    "    print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91      6998\n",
      "           1       0.00      0.00      0.00      1380\n",
      "\n",
      "    accuracy                           0.84      8378\n",
      "   macro avg       0.42      0.50      0.46      8378\n",
      "weighted avg       0.70      0.84      0.76      8378\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# dummy clf (always 0)\n",
    "print(classification_report(y, np.zeros(len(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
